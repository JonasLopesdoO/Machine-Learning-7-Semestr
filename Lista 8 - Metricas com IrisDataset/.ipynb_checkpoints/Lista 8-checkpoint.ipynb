{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolution with accuracy, precision, recall, f1_measure, log_loss and AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler #LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['sepal_length','sepal_width',\n",
    "           'petal_length', 'petal_width',\n",
    "           'type']\n",
    "df = pd.read_csv('iris.data', names=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width         type\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 5)"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      "sepal_length    150 non-null float64\n",
      "sepal_width     150 non-null float64\n",
      "petal_length    150 non-null float64\n",
      "petal_width     150 non-null float64\n",
      "type            150 non-null object\n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 5.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a66200a208>"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['type'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.hist(bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting label to numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "#le = LabelEncoder()\n",
    "#df['type'] = le.fit_transform(df.values[:, -1])\n",
    "\n",
    "# pd.get_dummies(df2['color'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.4</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.9</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.3</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>5.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>6.2</td>\n",
       "      <td>2.2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.1</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.2</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.2</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1.3</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>6.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>6.8</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.4</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>5.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.1</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.2</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.6</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.3</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.3</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>5.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.3</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    sepal_length  sepal_width  petal_length  petal_width             type\n",
       "0            5.1          3.5           1.4          0.2      Iris-setosa\n",
       "1            4.9          3.0           1.4          0.2      Iris-setosa\n",
       "2            4.7          3.2           1.3          0.2      Iris-setosa\n",
       "3            4.6          3.1           1.5          0.2      Iris-setosa\n",
       "4            5.0          3.6           1.4          0.2      Iris-setosa\n",
       "5            5.4          3.9           1.7          0.4      Iris-setosa\n",
       "6            4.6          3.4           1.4          0.3      Iris-setosa\n",
       "7            5.0          3.4           1.5          0.2      Iris-setosa\n",
       "8            4.4          2.9           1.4          0.2      Iris-setosa\n",
       "9            4.9          3.1           1.5          0.1      Iris-setosa\n",
       "10           5.4          3.7           1.5          0.2      Iris-setosa\n",
       "11           4.8          3.4           1.6          0.2      Iris-setosa\n",
       "12           4.8          3.0           1.4          0.1      Iris-setosa\n",
       "13           4.3          3.0           1.1          0.1      Iris-setosa\n",
       "14           5.8          4.0           1.2          0.2      Iris-setosa\n",
       "15           5.7          4.4           1.5          0.4      Iris-setosa\n",
       "16           5.4          3.9           1.3          0.4      Iris-setosa\n",
       "17           5.1          3.5           1.4          0.3      Iris-setosa\n",
       "18           5.7          3.8           1.7          0.3      Iris-setosa\n",
       "19           5.1          3.8           1.5          0.3      Iris-setosa\n",
       "20           5.4          3.4           1.7          0.2      Iris-setosa\n",
       "21           5.1          3.7           1.5          0.4      Iris-setosa\n",
       "22           4.6          3.6           1.0          0.2      Iris-setosa\n",
       "23           5.1          3.3           1.7          0.5      Iris-setosa\n",
       "24           4.8          3.4           1.9          0.2      Iris-setosa\n",
       "25           5.0          3.0           1.6          0.2      Iris-setosa\n",
       "26           5.0          3.4           1.6          0.4      Iris-setosa\n",
       "27           5.2          3.5           1.5          0.2      Iris-setosa\n",
       "28           5.2          3.4           1.4          0.2      Iris-setosa\n",
       "29           4.7          3.2           1.6          0.2      Iris-setosa\n",
       "..           ...          ...           ...          ...              ...\n",
       "60           5.0          2.0           3.5          1.0  Iris-versicolor\n",
       "61           5.9          3.0           4.2          1.5  Iris-versicolor\n",
       "62           6.0          2.2           4.0          1.0  Iris-versicolor\n",
       "63           6.1          2.9           4.7          1.4  Iris-versicolor\n",
       "64           5.6          2.9           3.6          1.3  Iris-versicolor\n",
       "65           6.7          3.1           4.4          1.4  Iris-versicolor\n",
       "66           5.6          3.0           4.5          1.5  Iris-versicolor\n",
       "67           5.8          2.7           4.1          1.0  Iris-versicolor\n",
       "68           6.2          2.2           4.5          1.5  Iris-versicolor\n",
       "69           5.6          2.5           3.9          1.1  Iris-versicolor\n",
       "70           5.9          3.2           4.8          1.8  Iris-versicolor\n",
       "71           6.1          2.8           4.0          1.3  Iris-versicolor\n",
       "72           6.3          2.5           4.9          1.5  Iris-versicolor\n",
       "73           6.1          2.8           4.7          1.2  Iris-versicolor\n",
       "74           6.4          2.9           4.3          1.3  Iris-versicolor\n",
       "75           6.6          3.0           4.4          1.4  Iris-versicolor\n",
       "76           6.8          2.8           4.8          1.4  Iris-versicolor\n",
       "77           6.7          3.0           5.0          1.7  Iris-versicolor\n",
       "78           6.0          2.9           4.5          1.5  Iris-versicolor\n",
       "79           5.7          2.6           3.5          1.0  Iris-versicolor\n",
       "80           5.5          2.4           3.8          1.1  Iris-versicolor\n",
       "81           5.5          2.4           3.7          1.0  Iris-versicolor\n",
       "82           5.8          2.7           3.9          1.2  Iris-versicolor\n",
       "83           6.0          2.7           5.1          1.6  Iris-versicolor\n",
       "84           5.4          3.0           4.5          1.5  Iris-versicolor\n",
       "85           6.0          3.4           4.5          1.6  Iris-versicolor\n",
       "86           6.7          3.1           4.7          1.5  Iris-versicolor\n",
       "87           6.3          2.3           4.4          1.3  Iris-versicolor\n",
       "88           5.6          3.0           4.1          1.3  Iris-versicolor\n",
       "89           5.5          2.5           4.0          1.3  Iris-versicolor\n",
       "\n",
       "[90 rows x 5 columns]"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:90]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spliting data to train, test with Stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.values[:, :-1]\n",
    "y = df.values[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.9, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15, 4), (135, 4), (15,), (135,))"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using GridSearchCV to test various hyperparameters already stratified with cross validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseEstimator.get_params of LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)>"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogisticRegression().get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score:  0.8666666666666667\n",
      "Best params:  {'logisticregression__C': 10.0, 'logisticregression__penalty': 'l1', 'logisticregression__tol': 0.0001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jonas Lopes\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\Jonas Lopes\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# using shuffle and random_random state in a stratified\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "pipe_Log = make_pipeline(StandardScaler(), LogisticRegression(multi_class='auto', solver='liblinear', max_iter=100)) #solver='lbfgs'\n",
    "\n",
    "param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 5.0, 10.0, 50.0, 100.0, 1000.00]\n",
    "\n",
    "param_grid = [{'logisticregression__C': param_range,\n",
    "               'logisticregression__tol': param_range,\n",
    "               'logisticregression__penalty': ['l1']},\n",
    "              {'logisticregression__C': param_range,\n",
    "               'logisticregression__tol': param_range,\n",
    "               'logisticregression__penalty': ['l2']}]\n",
    "\n",
    "gs = GridSearchCV(estimator=pipe_Log,\n",
    "                  param_grid=param_grid,\n",
    "                  scoring='accuracy',\n",
    "                  cv=cv,\n",
    "                  n_jobs=-1,\n",
    "                  return_train_score=True)\n",
    "\n",
    "gs = gs.fit(X_train, y_train)\n",
    "print('Best score: ', gs.best_score_)\n",
    "print('Best params: ', gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['param_logisticregression__C', 'param_logisticregression__tol', 'param_logisticregression__penalty', 'mean_train_score', 'mean_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_logisticregression__C</th>\n",
       "      <th>param_logisticregression__tol</th>\n",
       "      <th>param_logisticregression__penalty</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>l1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>50</td>\n",
       "      <td>0.001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>50</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>l1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>50</td>\n",
       "      <td>0.001</td>\n",
       "      <td>l1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>50</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>10</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>l2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>l2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>50</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.861111</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.861111</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>10</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>5</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>10</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>50</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>100</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0.001</td>\n",
       "      <td>5</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.001</td>\n",
       "      <td>10</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.001</td>\n",
       "      <td>50</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>5</td>\n",
       "      <td>1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>50</td>\n",
       "      <td>1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>10</td>\n",
       "      <td>1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    param_logisticregression__C param_logisticregression__tol  \\\n",
       "90                         1000                        0.0001   \n",
       "182                         100                          0.01   \n",
       "180                         100                        0.0001   \n",
       "93                         1000                           0.1   \n",
       "92                         1000                          0.01   \n",
       "91                         1000                         0.001   \n",
       "172                          50                          0.01   \n",
       "171                          50                         0.001   \n",
       "170                          50                        0.0001   \n",
       "62                           10                          0.01   \n",
       "82                          100                          0.01   \n",
       "81                          100                         0.001   \n",
       "80                          100                        0.0001   \n",
       "72                           50                          0.01   \n",
       "71                           50                         0.001   \n",
       "70                           50                        0.0001   \n",
       "60                           10                        0.0001   \n",
       "181                         100                         0.001   \n",
       "61                           10                         0.001   \n",
       "183                         100                           0.1   \n",
       "193                        1000                           0.1   \n",
       "192                        1000                          0.01   \n",
       "191                        1000                         0.001   \n",
       "190                        1000                        0.0001   \n",
       "73                           50                           0.1   \n",
       "42                            1                          0.01   \n",
       "43                            1                           0.1   \n",
       "160                          10                        0.0001   \n",
       "50                            5                        0.0001   \n",
       "161                          10                         0.001   \n",
       "..                          ...                           ...   \n",
       "105                      0.0001                             5   \n",
       "86                          100                            10   \n",
       "106                      0.0001                            10   \n",
       "107                      0.0001                            50   \n",
       "108                      0.0001                           100   \n",
       "109                      0.0001                          1000   \n",
       "115                       0.001                             5   \n",
       "116                       0.001                            10   \n",
       "117                       0.001                            50   \n",
       "87                          100                            50   \n",
       "85                          100                             5   \n",
       "47                            1                            50   \n",
       "65                           10                             5   \n",
       "48                            1                           100   \n",
       "49                            1                          1000   \n",
       "55                            5                             5   \n",
       "56                            5                            10   \n",
       "57                            5                            50   \n",
       "58                            5                           100   \n",
       "59                            5                          1000   \n",
       "66                           10                            10   \n",
       "79                           50                          1000   \n",
       "67                           10                            50   \n",
       "68                           10                           100   \n",
       "69                           10                          1000   \n",
       "75                           50                             5   \n",
       "76                           50                            10   \n",
       "77                           50                            50   \n",
       "78                           50                           100   \n",
       "199                        1000                          1000   \n",
       "\n",
       "    param_logisticregression__penalty  mean_train_score  mean_test_score  \n",
       "90                                 l1          1.000000         0.866667  \n",
       "182                                l2          1.000000         0.866667  \n",
       "180                                l2          1.000000         0.866667  \n",
       "93                                 l1          0.972222         0.866667  \n",
       "92                                 l1          1.000000         0.866667  \n",
       "91                                 l1          1.000000         0.866667  \n",
       "172                                l2          0.972222         0.866667  \n",
       "171                                l2          0.972222         0.866667  \n",
       "170                                l2          0.972222         0.866667  \n",
       "62                                 l1          0.972222         0.866667  \n",
       "82                                 l1          1.000000         0.866667  \n",
       "81                                 l1          1.000000         0.866667  \n",
       "80                                 l1          1.000000         0.866667  \n",
       "72                                 l1          1.000000         0.866667  \n",
       "71                                 l1          1.000000         0.866667  \n",
       "70                                 l1          1.000000         0.866667  \n",
       "60                                 l1          0.972222         0.866667  \n",
       "181                                l2          1.000000         0.866667  \n",
       "61                                 l1          0.972222         0.866667  \n",
       "183                                l2          0.972222         0.866667  \n",
       "193                                l2          0.972222         0.866667  \n",
       "192                                l2          1.000000         0.866667  \n",
       "191                                l2          1.000000         0.866667  \n",
       "190                                l2          1.000000         0.866667  \n",
       "73                                 l1          0.972222         0.800000  \n",
       "42                                 l1          0.861111         0.800000  \n",
       "43                                 l1          0.861111         0.800000  \n",
       "160                                l2          0.972222         0.800000  \n",
       "50                                 l1          0.972222         0.800000  \n",
       "161                                l2          0.972222         0.800000  \n",
       "..                                ...               ...              ...  \n",
       "105                                l2          0.333333         0.333333  \n",
       "86                                 l1          0.333333         0.333333  \n",
       "106                                l2          0.333333         0.333333  \n",
       "107                                l2          0.333333         0.333333  \n",
       "108                                l2          0.333333         0.333333  \n",
       "109                                l2          0.333333         0.333333  \n",
       "115                                l2          0.333333         0.333333  \n",
       "116                                l2          0.333333         0.333333  \n",
       "117                                l2          0.333333         0.333333  \n",
       "87                                 l1          0.333333         0.333333  \n",
       "85                                 l1          0.333333         0.333333  \n",
       "47                                 l1          0.333333         0.333333  \n",
       "65                                 l1          0.333333         0.333333  \n",
       "48                                 l1          0.333333         0.333333  \n",
       "49                                 l1          0.333333         0.333333  \n",
       "55                                 l1          0.333333         0.333333  \n",
       "56                                 l1          0.333333         0.333333  \n",
       "57                                 l1          0.333333         0.333333  \n",
       "58                                 l1          0.333333         0.333333  \n",
       "59                                 l1          0.333333         0.333333  \n",
       "66                                 l1          0.333333         0.333333  \n",
       "79                                 l1          0.333333         0.333333  \n",
       "67                                 l1          0.333333         0.333333  \n",
       "68                                 l1          0.333333         0.333333  \n",
       "69                                 l1          0.333333         0.333333  \n",
       "75                                 l1          0.333333         0.333333  \n",
       "76                                 l1          0.333333         0.333333  \n",
       "77                                 l1          0.333333         0.333333  \n",
       "78                                 l1          0.333333         0.333333  \n",
       "199                                l2          0.333333         0.333333  \n",
       "\n",
       "[200 rows x 5 columns]"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(gs.cv_results_)[cols].sort_values(by='mean_test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainning and predict with the best model finded to Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       " ('logisticregression',\n",
       "  LogisticRegression(C=10.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='auto',\n",
       "            n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "            tol=0.0001, verbose=0, warm_start=False))]"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_estimator_.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jonas Lopes\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\Jonas Lopes\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\Jonas Lopes\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "std = StandardScaler()\n",
    "X_train_std = std.fit_transform(X_train)\n",
    "X_test_std = std.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.941\n"
     ]
    }
   ],
   "source": [
    "lr_best = gs.best_estimator_\n",
    "lr_best.fit(X_train_std, y_train)\n",
    "y_pred = lr_best.predict(X_test_std)\n",
    "print('Test accuracy: %.3f' % lr_best.score(X_test_std, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      0.98      0.99        45\n",
      "Iris-versicolor       0.93      0.89      0.91        45\n",
      " Iris-virginica       0.90      0.96      0.92        45\n",
      "\n",
      "      micro avg       0.94      0.94      0.94       135\n",
      "      macro avg       0.94      0.94      0.94       135\n",
      "   weighted avg       0.94      0.94      0.94       135\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# in this case, the support will have the same value because the dataset is perfectly divided\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[45 45 45]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "precision,recall,fscore,support=score(y_test, y_pred, average=None)\n",
    "\n",
    "print(support)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[44  1  0]\n",
      " [ 0 40  5]\n",
      " [ 0  2 43]]\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accurcy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9407407407407408"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9407407407407408\n"
     ]
    }
   ],
   "source": [
    "accuracy = np.sum(np.diagonal(cm)) / np.sum(cm)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.93023256, 0.89583333])"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.precision_score(y_test, y_pred, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9420219638242894"
      ]
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.precision_score(y_test, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision = TP / (TP + FP) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.9302325581395349\n",
      "0.8958333333333334\n"
     ]
    }
   ],
   "source": [
    "precision = 0\n",
    "# c for 'coluna'\n",
    "for c in range(cm.shape[0]):\n",
    "    precision += (cm[c,c] / np.sum(cm[:, c])) * support[0]\n",
    "    print(cm[c,c] / np.sum(cm[:, c]))\n",
    "    \n",
    "\n",
    "precision = (precision / np.sum(support))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9420219638242894"
      ]
     },
     "execution_count": 514,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.97777778, 0.88888889, 0.95555556])"
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.recall_score(y_test, y_pred, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9407407407407408"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.recall_score(y_test, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall = TP / (TP + FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9777777777777777\n",
      "0.8888888888888888\n",
      "0.9555555555555556\n"
     ]
    }
   ],
   "source": [
    "recall = 0\n",
    "# l for 'linha'\n",
    "for l in range(cm.shape[0]):\n",
    "    recall += (cm[l,l] / np.sum(cm[l, :])) * support[0]\n",
    "    print(cm[l,l] / np.sum(cm[l, :]))\n",
    "    \n",
    "recall = (recall / np.sum(support))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9407407407407408"
      ]
     },
     "execution_count": 518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9408620456101429"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(y_test, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9413809163453186"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# because I already calculated the recall and precision with average = 'weighted',\n",
    "# I don't need to calculate f1 score with this weights\n",
    "\n",
    "f1 = 2 * ((precision * recall) / (precision + recall)) \n",
    "f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matriz de Probabilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Iris-setosa' 'Iris-versicolor' 'Iris-virginica']\n",
      "[[8.02476154e-01 1.97523619e-01 2.26896972e-07]\n",
      " [9.47865072e-01 5.21348809e-02 4.66583907e-08]\n",
      " [9.82097278e-01 1.79027007e-02 2.09758489e-08]\n",
      " [8.55672088e-01 1.44327775e-01 1.36949431e-07]\n",
      " [8.88828858e-03 7.84477274e-01 2.06634438e-01]]\n"
     ]
    }
   ],
   "source": [
    "y_score = lr_best.predict_proba(X_test_std)\n",
    "print(lr_best.classes_)\n",
    "print(y_score[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2826088988733995"
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.log_loss(y_test, y_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve and AUC to Iris-virgínica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Iris-setosa' 'Iris-versicolor' 'Iris-virginica']\n"
     ]
    }
   ],
   "source": [
    "print(lr_best.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC 0.9861728395061727\n"
     ]
    }
   ],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_score[:, 2], pos_label='Iris-virginica')\n",
    "print('AUC', metrics.auc(fpr, tpr))\n",
    "#metrics.auc(y_test, y_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a66200aef0>]"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([0, 1],[0, 1], '--') ## this create a ascending straight with --\n",
    "plt.plot(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another way to calculate the AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_binary = np.where(y_test == 'Iris-virginica', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9861728395061727"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.roc_auc_score(y_test_binary, y_score[:, 2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
