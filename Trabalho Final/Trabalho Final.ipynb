{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Work of Machine Learning Discipline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tipo de problema\n",
    "NER - Named Entity Recognition.\n",
    "\n",
    "Neste tipo de problema um modelo de predição para textos tentará predizer da melhor maneira possível quais são as entidades em um texto. Os problemas de NER surgem devido diversas maneiras de expressar uma determinada coisa e também onde uma coisa pode significar várias outras.\n",
    "A língua Portuguesa por exemplo possui muitos tempos verbais e maneiras de conjugação de verbos.\n",
    "\n",
    "Ex: conjugar, conjugado, conjugaria, conjugarei, conjugaríamos, conjugaríeis, conjugaste, conjugou, conjuguemos...\n",
    "\n",
    "Tente explicar por exemplo para um extrangeiro a diferença entre \"bota a calça e calça a bota\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problema\n",
    "O problema a ser resolvido com a resolução deste trabalho é utilizar um dataset com milhares de textos e palavras rotulados e a partir de um modelo de predições de texto realizar o ranqueamento das melhores classificações de entidades.\n",
    "\n",
    "Nós nos focaremos em 4 tipos de named entities: persons, locations, organizations e nomes de demais entidades que não pertençam as anteriores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "Será utilizado um dataset amplamente difundido que foi utilizado primariamente na Conference on Computational Natural Language Learning (CoNLL-2003) acessível a partir do seguinte link: https://www.clips.uantwerpen.be/conll2003/ner/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divisão do dataset\n",
    "O primeiro item de cada linha é uma palavra. O segundo é um Part-of-Speech (POS) tag. O terceiro é uma tag de fragmento sintático. A quarta é a tag de entidade nomeada.\n",
    "\n",
    "As tags de fragmento e os nomes de entidades tem o formato I-TYPE que significa que a palavra está dentro de uma frase do tipo TYPE.\n",
    "\n",
    " Uma palavra com a tag O não é parte de uma frase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divisão do dataset - rows\n",
    "Os arquivos de dados de tarefas compartilhadas CoNLL-2003 contêm quatro colunas separadas por um único espaço.\n",
    "\n",
    "- O primeiro item de cada linha no dataset é uma palavra.  \n",
    "- O segundo é um Part-of-Speech (POS) tag. \n",
    "- O terceiro é uma tag de fragmento sintático. \n",
    "- A quarta é a tag de entidade nomeada.\n",
    "\n",
    "As tags de fragmento e os nomes de entidades tem o formato I-TYPE que significa que a palavra está dentro de uma frase do tipo TYPE.\n",
    "\n",
    " Uma palavra com a tag O não é parte de uma frase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ferramenta\n",
    "Será utilizado o [Spacy](https://spacy.io/)\n",
    " que é uma ferramenta para o emprego de técnicas de Processamento de Linguagem Natural - PLN. Será utilizado voltado ao NER - Named Entity Recognition.\n",
    "\n",
    "O Spacy é uma ferramenta de código aberto muito poderosa que já possui diversos modelos prontos para uso para diversos idiomas.\n",
    "\n",
    "Será utilizado o modelo para classificação de textos em inglês.\n",
    "\n",
    "Algumas funções do spacy estão definidas mais abaixo no documento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#U.N.         NNP  I-NP  I-ORG \n",
    "#official     NN   I-NP  O \n",
    "#Ekeus        NNP  I-NP  I-PER \n",
    "#heads        VBZ  I-VP  O \n",
    "#for          IN   I-PP  O \n",
    "#Baghdad      NNP  I-NP  I-LOC \n",
    "#.            .    O     O "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divisão dos arquivos de dados\n",
    "A divisão dos arquivos do dataset se dão da seguinte forma:\n",
    "    - Consiste de 3 arquivos por linguagem\n",
    "    - Um arquivo de treino\n",
    "    - Dois arquivos de teste, testeA e testeB\n",
    "    - O primeiro arquivo de teste será usado em produção para encontrar os melhores parametros\n",
    "    - O segundo arquivo de teste será usado para a avaliação final\n",
    "    - Os dados estão disponíveis em dois datasets, um em Inglês e também em Alemão. Para o propósito deste trabalho será usado apenas a versão em inglês.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arquivos\n",
    "- eng.raw.tar   - 13,930 MB - Conjunto de dados em inglês\n",
    "- ner.tgz       - 3,374  MB - Contém o software para fazer o build dos dados\n",
    "- 000README.txt - 8      KB - Instruções para descompactação\n",
    "\n",
    "Os dados em inglês são uma coleção de artigos de notícias do Reuters Corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métricas de Avaliação\n",
    "A competição utiliza três métricas principais que são:\n",
    "- Precision\n",
    "- Recall\n",
    "- F-Score\n",
    "\n",
    "Precision é a porcentagem de named entities encontradas pelo sistema de aprendizado que estão corretas.\n",
    "\n",
    "Recall é a porcentagem de named entities presentes no Corpus que são encontradas pelo sistema.\n",
    "\n",
    "Uma named entity só está correta se for uma correspondência exata da entidade correspondente no\n",
    "arquivo de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data: Examples and their annotations.\n",
    "\n",
    "### Text: The input text the model should predict a label for.\n",
    "\n",
    "### Label: The label the model should predict.\n",
    "\n",
    "### Gradient: Gradient of the loss function calculating the difference between input and expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename=\"training model.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.tokens import Span, Doc\n",
    "from spacy.vocab import Vocab\n",
    "from spacy.pipeline import EntityRuler\n",
    "from spacy.lang.en import English\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'noun, proper singular'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mostra o que significa cada saída do token\n",
    "spacy.explain(\"NNP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple PROPN nsubj\n",
      "is VERB aux\n",
      "looking VERB ROOT\n",
      "at ADP prep\n",
      "buying VERB pcomp\n",
      "U.K. PROPN compound\n",
      "startup NOUN dobj\n",
      "for ADP prep\n",
      "$ SYM quantmod\n",
      "1 NUM compound\n",
      "billion NUM pobj\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Apple PROPN NNP nsubj Xxxxx True False\n",
      "is be VERB VBZ aux xx True True\n",
      "looking look VERB VBG ROOT xxxx True False\n",
      "at at ADP IN prep xx True True\n",
      "buying buy VERB VBG pcomp xxxx True False\n",
      "U.K. U.K. PROPN NNP compound X.X. False False\n",
      "startup startup NOUN NN dobj xxxx True False\n",
      "for for ADP IN prep xxx True True\n",
      "$ $ SYM $ quantmod $ False False\n",
      "1 1 NUM CD compound d False False\n",
      "billion billion NUM CD pobj xxxx True False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separando cada parte do texto como string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp(u'You finded the book that I told you, Carla?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You', 'finded', 'the', 'book', 'that', 'I', 'told', 'you,', 'Carla?']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2.text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pegando cada token e mostrando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[token for token in doc2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pegando os tokens como strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[token.orth_ for token in doc2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pegando apenas as palavras de um texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[token.orth_ for token in doc2 if not token.is_punct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[token.orth_ for token in doc2 if token.is_punct]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similaridade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [token for token in doc2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you | I\n",
    "tokens[0].similarity(tokens[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you | book\n",
    "tokens[0].similarity(tokens[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise de classes gramaticais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('You', 'PRON'),\n",
       " ('finded', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('book', 'NOUN'),\n",
       " ('that', 'DET'),\n",
       " ('I', 'PRON'),\n",
       " ('told', 'VERB'),\n",
       " ('you', 'PRON'),\n",
       " (',', 'PUNCT'),\n",
       " ('Carla', 'PROPN'),\n",
       " ('?', 'PUNCT')]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(token.orth_, token.pos_) for token in doc2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fazendo a lematização \n",
    "Isso funciona para textos que podem ter diversas conjufações e tempos verbais, assim ele somente pega a raíz da palavra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['find', 'tell']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.lemma_ for token in doc2 if token.pos_ == 'VERB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp(u'knew, know')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.lemma_ for token in doc2 if token.pos_ == 'VERB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifica se uma palavra é ancestral de outra\n",
    "doc2 = nlp(u'find finded')\n",
    "tokens = [token for token in doc2]\n",
    "tokens[0].is_ancestor(tokens[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entidades!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u'United States')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(entity, entity.label_) for entity in doc.ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exemplo maior\n",
    "wiki_obama = \"\"\"Barack Obama is an American politician who served as\n",
    "the 44th President of the United States from 2009 to 2017. He is the first\n",
    "African American to have served as president,\n",
    "as well as the first born outside the contiguous United States.\"\"\"\n",
    "nlp_obama = nlp(wiki_obama)\n",
    "[(entity, entity.label_) for entity in nlp_obama.ents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mostrando informações sobre cada entidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "doc = nlp(u\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "doc = nlp(u\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "spacy.displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dep = nlp(u\"This is a sentence.\")\n",
    "displacy.serve(doc_dep, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pegando tokens, substantivos e sentenças"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u\"Peach emoji is where it has always been. Peach is the superior \"\n",
    "          u\"emoji. It's outranking eggplant 🍑 \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc[0].text)          # 'Peach'\n",
    "print(doc[1].text)          # 'emoji'\n",
    "print(doc[-1].text)         # '🍑'\n",
    "print(doc[17:19].text)      # 'outranking eggplant'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_chunks = list(doc.noun_chunks)\n",
    "print(noun_chunks[0].text)  # 'Peach emoji'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentences\n",
    "sentences = list(doc.sents)\n",
    "assert len(sentences) == 3\n",
    "print (sentences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get part-of-speech tags and flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "apple = doc[0]\n",
    "print(\"Fine-grained POS tag:\", apple.pos_, apple.pos)\n",
    "print(\"Coarse-grained POS tag:\", apple.tag_, apple.tag)\n",
    "print(\"Word shape:\", apple.shape_, apple.shape)\n",
    "print(\"Alphanumeric characters?:\", apple.is_alpha)\n",
    "print(\"Punctuation mark?:\", apple.is_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "billion = doc[10]\n",
    "print(\"Digit?\", billion.is_digit)\n",
    "print(\"Like a number?\", billion.like_num)\n",
    "print(\"Like an email address?\", billion.like_email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconheça e atualize named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u\"San Francisco considers banning sidewalk delivery robots\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain(\"GPE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u\"FB is hiring a new VP of global policy\")\n",
    "doc.ents = [Span(doc, 0, 1, label=doc.vocab.strings[u\"ORG\"])]\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando e atualizando modelos de rede neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "train_data = [(u\"Uber blew through $1 million\", {\"entities\": [(0, 4, \"ORG\")]})]\n",
    "\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "\n",
    "with nlp.disable_pipes(*other_pipes):\n",
    "    optimizer = nlp.begin_training()\n",
    "    for i in range(10):\n",
    "        random.shuffle(train_data)\n",
    "        for text, annotations in train_data:\n",
    "            nlp.update([text], [annotations], sgd=optimizer)\n",
    "            \n",
    "# salvando o modelo no disco         \n",
    "nlp.to_disk(\"models/modelo.bin\")\n",
    "# trazendo o modelo de volta ao disco\n",
    "nlp = spacy.load(\"models/modelo.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilizando o novo modelo\n",
    "customer_feedback = open(\"models/customer_feedback_627.txt\").read()\n",
    "doc = nlp(customer_feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain(\"U.S.A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialização simples e eficiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(U.S.A.,)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.tokens import Doc\n",
    "from spacy.vocab import Vocab\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "customer_feedback = open(\"models/customer_feedback_627.txt\").read()\n",
    "doc = nlp(customer_feedback)\n",
    "\n",
    "doc.to_disk(\"models/customer_feedback_627.bin\")\n",
    "\n",
    "new_doc = Doc(Vocab()).from_disk(\"models/customer_feedback_627.bin\")\n",
    "\n",
    "new_doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain(\"dobj\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matcher Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York\n"
     ]
    }
   ],
   "source": [
    "# Matcher para palavras\n",
    "\n",
    "# Matcher is initialized with the shared vocab\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Each dict represents one token and its attributes\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add with ID, optional callback and pattern(s)\n",
    "pattern = [{\"LOWER\": \"new\"}, {\"LOWER\": \"york\"}]\n",
    "matcher.add('CITIES', None, pattern)\n",
    "\n",
    "# Match by calling the matcher on a Doc object\n",
    "doc = nlp(\"I live in New York\")\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Matches are (match_id, start, end) tuples\n",
    "for match_id, start, end in matches:\n",
    "     # Get the matched span by slicing the Doc\n",
    "    span = doc[start:end]\n",
    "    print(span.text)\n",
    "# 'New York'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizando Match Pattern para criar um modelo de NER com Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "818268"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = open(\"dados/eng.train.txt\").read()\n",
    "train = train.split()\n",
    "len(train)\n",
    "# O spacy só aceita por vez 100000 caracteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'patterns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-3ef19f2e80ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mi\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m#print(\"Pattern\",train[i] + \" Label\",train[i+3])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mpatterns\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"label\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"pattern\"\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mi\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'patterns' is not defined"
     ]
    }
   ],
   "source": [
    "#Tentativa com arquivo de treino\n",
    "nlp = English();\n",
    "ruler = EntityRuler(nlp)\n",
    "\n",
    "i = 0\n",
    "\n",
    "while(i <= len(train)-4):\n",
    "    if train[i] == '-DOCSTART-':\n",
    "        i += 4\n",
    "    #print(\"Pattern\",train[i] + \" Label\",train[i+3])   \n",
    "    patterns += [{\"label\": train[i+3], \"pattern\" : train[i]}]\n",
    "\n",
    "    i += 4\n",
    "\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "#adicionando o pipe ruler dentro do nlp\n",
    "nlp.add_pipe(ruler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ruler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-f6517c577a28>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Enviando o ruler para o disco e salvando-o.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mruler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_disk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"models/patterns.json\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'ruler' is not defined"
     ]
    }
   ],
   "source": [
    "# Enviando o ruler para o disco e salvando-o.\n",
    "ruler.to_disk(\"models/patterns.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Apple', 'I-ORG'), ('.', 'I-LOC'), ('The', 'I-LOC'), ('Apple', 'I-ORG'), ('is', 'I-MISC'), ('opening', 'O'), ('its', 'O'), ('first', 'O'), ('big', 'O'), ('office', 'O'), ('in', 'O'), ('San', 'I-LOC'), ('Francisco', 'I-LOC'), ('.', 'I-LOC')]\n"
     ]
    }
   ],
   "source": [
    "nlp = English()\n",
    "\n",
    "ruler_disk = EntityRuler(nlp).from_disk(\"models/patterns.json\")\n",
    "\n",
    "nlp.add_pipe(ruler_disk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u\"EU rejects German call to boycott British lamb.\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('MyCorp Inc.', 'I-ORG'), ('U.S.', 'GPE'), ('Microsoft', 'ORG')]\n"
     ]
    }
   ],
   "source": [
    "# usando um modelo já pronto do Spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "ruler = EntityRuler(nlp, overwrite_ents=True)\n",
    "patterns = [{\"label\": \"I-ORG\", \"pattern\": \"MyCorp Inc.\"}]\n",
    "ruler.add_patterns(patterns)\n",
    "nlp.add_pipe(ruler)\n",
    "\n",
    "doc = nlp(u\"MyCorp Inc. is a company in the U.S., Microsoft too\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match text com regras de tokens para analise de emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "def set_sentiment(matcher, doc, i, matches):\n",
    "    doc.sentiment += 0.1\n",
    "\n",
    "pattern1 = [{\"ORTH\": \"Google\"}, {\"ORTH\": \"I\"}, {\"ORTH\": \"/\"}, {\"ORTH\": \"O\"}]\n",
    "pattern2 = [[{\"ORTH\": emoji, \"OP\": \"+\"}] for emoji in [\"😀\", \"😂\", \"🤣\", \"😍\"]]\n",
    "matcher.add(\"GoogleIO\", None, pattern1)  # Match \"Google I/O\" or \"Google i/o\"\n",
    "matcher.add(\"HAPPY\", set_sentiment, *pattern2)  # Match one or more happy emoji\n",
    "\n",
    "doc = nlp(u\"A text about Google I/O 😀😀\")\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]\n",
    "    span = doc[start:end]\n",
    "    print(string_id, span.text)\n",
    "print(\"Sentiment\", doc.sentiment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
