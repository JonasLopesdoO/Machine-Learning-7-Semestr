{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Work of Machine Learning Discipline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tipo de problema\n",
    "NER - Named Entity Recognition.\n",
    "\n",
    "Neste tipo de problema um modelo de predi√ß√£o para textos tentar√° predizer da melhor maneira poss√≠vel quais s√£o as entidades em um texto. Os problemas de NER surgem devido diversas maneiras de expressar uma determinada coisa e tamb√©m onde uma coisa pode significar v√°rias outras.\n",
    "A l√≠ngua Portuguesa por exemplo possui muitos tempos verbais e maneiras de conjuga√ß√£o de verbos.\n",
    "\n",
    "Ex: conjugar, conjugado, conjugaria, conjugarei, conjugar√≠amos, conjugar√≠eis, conjugaste, conjugou, conjuguemos...\n",
    "\n",
    "Tente explicar por exemplo para um extrangeiro a diferen√ßa entre \"bota a cal√ßa e cal√ßa a bota\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problema\n",
    "O problema a ser resolvido com a resolu√ß√£o deste trabalho √© utilizar um dataset com milhares de textos e palavras rotulados e a partir de um modelo de predi√ß√µes de texto realizar o ranqueamento das melhores classifica√ß√µes de entidades.\n",
    "\n",
    "N√≥s nos focaremos em 4 tipos de named entities: persons, locations, organizations e nomes de demais entidades que n√£o perten√ßam as anteriores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "Ser√° utilizado um dataset amplamente difundido que foi utilizado primariamente na Conference on Computational Natural Language Learning (CoNLL-2003) acess√≠vel a partir do seguinte link: https://www.clips.uantwerpen.be/conll2003/ner/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divis√£o do dataset\n",
    "O primeiro item de cada linha √© uma palavra. O segundo √© um Part-of-Speech (POS) tag. O terceiro √© uma tag de fragmento sint√°tico. A quarta √© a tag de entidade nomeada.\n",
    "\n",
    "As tags de fragmento e os nomes de entidades tem o formato I-TYPE que significa que a palavra est√° dentro de uma frase do tipo TYPE.\n",
    "\n",
    " Uma palavra com a tag O n√£o √© parte de uma frase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divis√£o do dataset - rows\n",
    "Os arquivos de dados de tarefas compartilhadas CoNLL-2003 cont√™m quatro colunas separadas por um √∫nico espa√ßo.\n",
    "\n",
    "- O primeiro item de cada linha no dataset √© uma palavra.  \n",
    "- O segundo √© um Part-of-Speech (POS) tag. \n",
    "- O terceiro √© uma tag de fragmento sint√°tico. \n",
    "- A quarta √© a tag de entidade nomeada.\n",
    "\n",
    "As tags de fragmento e os nomes de entidades tem o formato I-TYPE que significa que a palavra est√° dentro de uma frase do tipo TYPE.\n",
    "\n",
    " Uma palavra com a tag O n√£o √© parte de uma frase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ferramenta\n",
    "Ser√° utilizado o [Spacy](https://spacy.io/)\n",
    " que √© uma ferramenta para o emprego de t√©cnicas de Processamento de Linguagem Natural - PLN. Ser√° utilizado voltado ao NER - Named Entity Recognition.\n",
    "\n",
    "O Spacy √© uma ferramenta de c√≥digo aberto muito poderosa que j√° possui diversos modelos prontos para uso para diversos idiomas.\n",
    "\n",
    "Ser√° utilizado o modelo para classifica√ß√£o de textos em ingl√™s.\n",
    "\n",
    "Algumas fun√ß√µes do spacy est√£o definidas mais abaixo no documento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#U.N.         NNP  I-NP  I-ORG \n",
    "#official     NN   I-NP  O \n",
    "#Ekeus        NNP  I-NP  I-PER \n",
    "#heads        VBZ  I-VP  O \n",
    "#for          IN   I-PP  O \n",
    "#Baghdad      NNP  I-NP  I-LOC \n",
    "#.            .    O     O "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divis√£o dos arquivos de dados\n",
    "A divis√£o dos arquivos do dataset se d√£o da seguinte forma:\n",
    "    - Consiste de 3 arquivos por linguagem\n",
    "    - Um arquivo de treino\n",
    "    - Dois arquivos de teste, testeA e testeB\n",
    "    - O primeiro arquivo de teste ser√° usado em produ√ß√£o para encontrar os melhores parametros\n",
    "    - O segundo arquivo de teste ser√° usado para a avalia√ß√£o final\n",
    "    - Os dados est√£o dispon√≠veis em dois datasets, um em Ingl√™s e tamb√©m em Alem√£o. Para o prop√≥sito deste trabalho ser√° usado apenas a vers√£o em ingl√™s.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arquivos\n",
    "- eng.raw.tar   - 13,930 MB - Conjunto de dados em ingl√™s\n",
    "- ner.tgz       - 3,374  MB - Cont√©m o software para fazer o build dos dados\n",
    "- 000README.txt - 8      KB - Instru√ß√µes para descompacta√ß√£o\n",
    "\n",
    "Os dados em ingl√™s s√£o uma cole√ß√£o de artigos de not√≠cias do Reuters Corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M√©tricas de Avalia√ß√£o\n",
    "A competi√ß√£o utiliza tr√™s m√©tricas principais que s√£o:\n",
    "- Precision\n",
    "- Recall\n",
    "- F-Score\n",
    "\n",
    "Precision √© a porcentagem de named entities encontradas pelo sistema de aprendizado que est√£o corretas.\n",
    "\n",
    "Recall √© a porcentagem de named entities presentes no Corpus que s√£o encontradas pelo sistema.\n",
    "\n",
    "Uma named entity s√≥ est√° correta se for uma correspond√™ncia exata da entidade correspondente no\n",
    "arquivo de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data: Examples and their annotations.\n",
    "\n",
    "### Text: The input text the model should predict a label for.\n",
    "\n",
    "### Label: The label the model should predict.\n",
    "\n",
    "### Gradient: Gradient of the loss function calculating the difference between input and expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename=\"training model.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.tokens import Span, Doc\n",
    "from spacy.vocab import Vocab\n",
    "from spacy.pipeline import EntityRuler\n",
    "from spacy.lang.en import English\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'noun, proper singular'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mostra o que significa cada sa√≠da do token\n",
    "spacy.explain(\"NNP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple PROPN nsubj\n",
      "is VERB aux\n",
      "looking VERB ROOT\n",
      "at ADP prep\n",
      "buying VERB pcomp\n",
      "U.K. PROPN compound\n",
      "startup NOUN dobj\n",
      "for ADP prep\n",
      "$ SYM quantmod\n",
      "1 NUM compound\n",
      "billion NUM pobj\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Apple PROPN NNP nsubj Xxxxx True False\n",
      "is be VERB VBZ aux xx True True\n",
      "looking look VERB VBG ROOT xxxx True False\n",
      "at at ADP IN prep xx True True\n",
      "buying buy VERB VBG pcomp xxxx True False\n",
      "U.K. U.K. PROPN NNP compound X.X. False False\n",
      "startup startup NOUN NN dobj xxxx True False\n",
      "for for ADP IN prep xxx True True\n",
      "$ $ SYM $ quantmod $ False False\n",
      "1 1 NUM CD compound d False False\n",
      "billion billion NUM CD pobj xxxx True False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separando cada parte do texto como string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp(u'You finded the book that I told you, Carla?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You', 'finded', 'the', 'book', 'that', 'I', 'told', 'you,', 'Carla?']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2.text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pegando cada token e mostrando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[token for token in doc2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pegando os tokens como strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[token.orth_ for token in doc2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pegando apenas as palavras de um texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[token.orth_ for token in doc2 if not token.is_punct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[token.orth_ for token in doc2 if token.is_punct]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similaridade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [token for token in doc2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you | I\n",
    "tokens[0].similarity(tokens[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you | book\n",
    "tokens[0].similarity(tokens[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An√°lise de classes gramaticais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('You', 'PRON'),\n",
       " ('finded', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('book', 'NOUN'),\n",
       " ('that', 'DET'),\n",
       " ('I', 'PRON'),\n",
       " ('told', 'VERB'),\n",
       " ('you', 'PRON'),\n",
       " (',', 'PUNCT'),\n",
       " ('Carla', 'PROPN'),\n",
       " ('?', 'PUNCT')]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(token.orth_, token.pos_) for token in doc2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fazendo a lematiza√ß√£o \n",
    "Isso funciona para textos que podem ter diversas conjufa√ß√µes e tempos verbais, assim ele somente pega a ra√≠z da palavra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['find', 'tell']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.lemma_ for token in doc2 if token.pos_ == 'VERB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp(u'knew, know')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.lemma_ for token in doc2 if token.pos_ == 'VERB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifica se uma palavra √© ancestral de outra\n",
    "doc2 = nlp(u'find finded')\n",
    "tokens = [token for token in doc2]\n",
    "tokens[0].is_ancestor(tokens[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entidades!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u'United States')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(entity, entity.label_) for entity in doc.ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exemplo maior\n",
    "wiki_obama = \"\"\"Barack Obama is an American politician who served as\n",
    "the 44th President of the United States from 2009 to 2017. He is the first\n",
    "African American to have served as president,\n",
    "as well as the first born outside the contiguous United States.\"\"\"\n",
    "nlp_obama = nlp(wiki_obama)\n",
    "[(entity, entity.label_) for entity in nlp_obama.ents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mostrando informa√ß√µes sobre cada entidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "doc = nlp(u\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "doc = nlp(u\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "spacy.displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dep = nlp(u\"This is a sentence.\")\n",
    "displacy.serve(doc_dep, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pegando tokens, substantivos e senten√ßas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u\"Peach emoji is where it has always been. Peach is the superior \"\n",
    "          u\"emoji. It's outranking eggplant üçë \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc[0].text)          # 'Peach'\n",
    "print(doc[1].text)          # 'emoji'\n",
    "print(doc[-1].text)         # 'üçë'\n",
    "print(doc[17:19].text)      # 'outranking eggplant'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_chunks = list(doc.noun_chunks)\n",
    "print(noun_chunks[0].text)  # 'Peach emoji'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentences\n",
    "sentences = list(doc.sents)\n",
    "assert len(sentences) == 3\n",
    "print (sentences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get part-of-speech tags and flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "apple = doc[0]\n",
    "print(\"Fine-grained POS tag:\", apple.pos_, apple.pos)\n",
    "print(\"Coarse-grained POS tag:\", apple.tag_, apple.tag)\n",
    "print(\"Word shape:\", apple.shape_, apple.shape)\n",
    "print(\"Alphanumeric characters?:\", apple.is_alpha)\n",
    "print(\"Punctuation mark?:\", apple.is_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "billion = doc[10]\n",
    "print(\"Digit?\", billion.is_digit)\n",
    "print(\"Like a number?\", billion.like_num)\n",
    "print(\"Like an email address?\", billion.like_email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconhe√ßa e atualize named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u\"San Francisco considers banning sidewalk delivery robots\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain(\"GPE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u\"FB is hiring a new VP of global policy\")\n",
    "doc.ents = [Span(doc, 0, 1, label=doc.vocab.strings[u\"ORG\"])]\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando e atualizando modelos de rede neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "train_data = [(u\"Uber blew through $1 million\", {\"entities\": [(0, 4, \"ORG\")]})]\n",
    "\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "\n",
    "with nlp.disable_pipes(*other_pipes):\n",
    "    optimizer = nlp.begin_training()\n",
    "    for i in range(10):\n",
    "        random.shuffle(train_data)\n",
    "        for text, annotations in train_data:\n",
    "            nlp.update([text], [annotations], sgd=optimizer)\n",
    "            \n",
    "# salvando o modelo no disco         \n",
    "nlp.to_disk(\"models/modelo.bin\")\n",
    "# trazendo o modelo de volta ao disco\n",
    "nlp = spacy.load(\"models/modelo.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilizando o novo modelo\n",
    "customer_feedback = open(\"models/customer_feedback_627.txt\").read()\n",
    "doc = nlp(customer_feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain(\"U.S.A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serializa√ß√£o simples e eficiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(U.S.A.,)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.tokens import Doc\n",
    "from spacy.vocab import Vocab\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "customer_feedback = open(\"models/customer_feedback_627.txt\").read()\n",
    "doc = nlp(customer_feedback)\n",
    "\n",
    "doc.to_disk(\"models/customer_feedback_627.bin\")\n",
    "\n",
    "new_doc = Doc(Vocab()).from_disk(\"models/customer_feedback_627.bin\")\n",
    "\n",
    "new_doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain(\"dobj\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matcher Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York\n"
     ]
    }
   ],
   "source": [
    "# Matcher para palavras\n",
    "\n",
    "# Matcher is initialized with the shared vocab\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Each dict represents one token and its attributes\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add with ID, optional callback and pattern(s)\n",
    "pattern = [{\"LOWER\": \"new\"}, {\"LOWER\": \"york\"}]\n",
    "matcher.add('CITIES', None, pattern)\n",
    "\n",
    "# Match by calling the matcher on a Doc object\n",
    "doc = nlp(\"I live in New York\")\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Matches are (match_id, start, end) tuples\n",
    "for match_id, start, end in matches:\n",
    "     # Get the matched span by slicing the Doc\n",
    "    span = doc[start:end]\n",
    "    print(span.text)\n",
    "# 'New York'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizando Match Pattern para criar um modelo de NER com Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "818268"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = open(\"dados/eng.train.txt\").read()\n",
    "train = train.split()\n",
    "len(train)\n",
    "# O spacy s√≥ aceita por vez 100000 caracteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'patterns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-3ef19f2e80ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mi\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m#print(\"Pattern\",train[i] + \" Label\",train[i+3])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mpatterns\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"label\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"pattern\"\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mi\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'patterns' is not defined"
     ]
    }
   ],
   "source": [
    "#Tentativa com arquivo de treino\n",
    "nlp = English();\n",
    "ruler = EntityRuler(nlp)\n",
    "\n",
    "i = 0\n",
    "\n",
    "while(i <= len(train)-4):\n",
    "    if train[i] == '-DOCSTART-':\n",
    "        i += 4\n",
    "    #print(\"Pattern\",train[i] + \" Label\",train[i+3])   \n",
    "    patterns += [{\"label\": train[i+3], \"pattern\" : train[i]}]\n",
    "\n",
    "    i += 4\n",
    "\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "#adicionando o pipe ruler dentro do nlp\n",
    "nlp.add_pipe(ruler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ruler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-f6517c577a28>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Enviando o ruler para o disco e salvando-o.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mruler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_disk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"models/patterns.json\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'ruler' is not defined"
     ]
    }
   ],
   "source": [
    "# Enviando o ruler para o disco e salvando-o.\n",
    "ruler.to_disk(\"models/patterns.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Apple', 'I-ORG'), ('.', 'I-LOC'), ('The', 'I-LOC'), ('Apple', 'I-ORG'), ('is', 'I-MISC'), ('opening', 'O'), ('its', 'O'), ('first', 'O'), ('big', 'O'), ('office', 'O'), ('in', 'O'), ('San', 'I-LOC'), ('Francisco', 'I-LOC'), ('.', 'I-LOC')]\n"
     ]
    }
   ],
   "source": [
    "nlp = English()\n",
    "\n",
    "ruler_disk = EntityRuler(nlp).from_disk(\"models/patterns.json\")\n",
    "\n",
    "nlp.add_pipe(ruler_disk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u\"EU rejects German call to boycott British lamb.\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('MyCorp Inc.', 'I-ORG'), ('U.S.', 'GPE'), ('Microsoft', 'ORG')]\n"
     ]
    }
   ],
   "source": [
    "# usando um modelo j√° pronto do Spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "ruler = EntityRuler(nlp, overwrite_ents=True)\n",
    "patterns = [{\"label\": \"I-ORG\", \"pattern\": \"MyCorp Inc.\"}]\n",
    "ruler.add_patterns(patterns)\n",
    "nlp.add_pipe(ruler)\n",
    "\n",
    "doc = nlp(u\"MyCorp Inc. is a company in the U.S., Microsoft too\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match text com regras de tokens para analise de emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "def set_sentiment(matcher, doc, i, matches):\n",
    "    doc.sentiment += 0.1\n",
    "\n",
    "pattern1 = [{\"ORTH\": \"Google\"}, {\"ORTH\": \"I\"}, {\"ORTH\": \"/\"}, {\"ORTH\": \"O\"}]\n",
    "pattern2 = [[{\"ORTH\": emoji, \"OP\": \"+\"}] for emoji in [\"üòÄ\", \"üòÇ\", \"ü§£\", \"üòç\"]]\n",
    "matcher.add(\"GoogleIO\", None, pattern1)  # Match \"Google I/O\" or \"Google i/o\"\n",
    "matcher.add(\"HAPPY\", set_sentiment, *pattern2)  # Match one or more happy emoji\n",
    "\n",
    "doc = nlp(u\"A text about Google I/O üòÄüòÄ\")\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]\n",
    "    span = doc[start:end]\n",
    "    print(string_id, span.text)\n",
    "print(\"Sentiment\", doc.sentiment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
